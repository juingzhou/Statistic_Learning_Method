# KNN
###1. K近邻算法
 - k近邻算法是一种基本的分类算法, 他的思想非常的简单直观, 即一个样本的类别应该和训练数据集中和它距离最近的k个样本中多个样本所属的类别相同, 因此, k近邻分类时没有显式的学习过程。k近邻的模型实际是一种对特征空间的划分, 模型由距离度量、k值选择和决策规则决定, 对于决策规则, 我们一般使用多数表决的原则, 因此模型的表现主要有距离度量和k值决定。
####1.1 距离度量
 - 特征空间中两点间的距离可以看作是两个样本相似度的一种表现, k近邻模型的特征空间一般是n维实数向量空间$R^n$。使用的距离是欧式距离,但也可以使用其他距离, 比如更一般的$L_p$距离($L_p distance$), 或者`Minkowski`距离。
 - 设特征空间$X$是n维实数向量空间$R^n,x(i),x(j) \in X, x=(x_0,x_1,x_2,......,x_n)^T,x(i),x(j)$的$L_p$距离定义为:
 $$ \mathbb{L_p}(x_i,x_j) = (\sum_{k=0}^{n}|x_i^{(k)} - x_j^{(k)}|^p)^ \frac{1}p $$  
 这里$p \geq 1$。当$p=2$时, 称为欧式距离$(Euclidean distance)$，即
 $$ \mathbb{L_2}(x_i,x_j) = (\sum_{k=0}^{n}|x_i^{(k)} - x_j^{(k)}|^2)^ \frac{1}2 $$  
 当$p=1$时, 称为曼哈顿距离$(Manhanttan distance)$, 即
  $$ \mathbb{L_1}(x_i,x_j) = \max_{k=1}\sum_{k=0}^{n}n|x_i^{(k)} - x_j^{(k)}|$$  
当$p=\infty$时, 它是各个坐标距离的最大值,即:
  $$ \mathbb{L_\infty}(x_i,x_j) = \max_{k}|x_i^{(k)} - x_j^{(k)}|$$ 
 
####1.2 k值的选择

 - k值的选择对于模型的表现具有非常重要的影响。具体来说，如果选择一个较小的k值,就是用一个较小的邻域中的训练实例进行预测，这种情况下“学习”的近似误差会很小，但是“学习”的估计误差会增大。因为只使用一个较小的邻域去预测，训练集中的噪声点将会对结果造成很大的影响，考虑一个极端情况，当k=1时，预测样本的类别就等于训练集中与它距离最近的样本的类别，如果该点刚好是噪声点，那么预测将会发生错误。也就是说，k越小，模型越复杂，也就越容易发生过拟合。 
 - 相反的，当我们增大k值时，模型会变得简单，相应的容易出现欠拟合。考虑当k=n是，训练集中所有的点均是输入实例的邻域，对于任何输入实例其分类均为训练集中多数样本所属的类别。 
 - 因此在实际应用中，k通常取一个比较小的值，但同时也要通过交叉验证等方式确定k的具体取值。
### 2.k近邻算法
- 输入: 训练集$T={(x{(1)},y{(1)}),(x{(2)},y{(2)}),...,(x{(m)},y{(m)})}$其中$x{(i)}\in X= R^n$为实例的特征向量，$y^{(i)}\in Y={c_1, c_2, ...,c_t}$为实例的类别, $i=1,2,...,m$某个实例特征向量$x$。
- 输出: 实例$x$的所属类别$y$  
    - 根据给定的距离度量, 在训练集$T$中找出与$x$最近邻的$k$个点, 涵盖这$k$个点的林奈于记作$N_k(x)$
    - 在$N_k(x)$中根据分类决策规则(如多数表决)决定$x$的类别$$y=arg \max_{c_j} \displaystyle\sum_{x_i\in N_k (x)} I(y_i=c_j)$$
    其中$i=1,2,.....,m$, $j=1,2,...,t$, $I$为指示函数, 即当$y_i = c_j$时为1，否则为0

- k近邻的特殊情况时$k=1$的情形, 称为最近邻算法。遂于输入的实例点$x$，在最近邻算法将训练数据集中与$x$最近的类作为$x$的类
- k的选择会对k近邻法的结果产生重大影响.
- 如果选择较小的k值, "学习"的估计误差会很大, 预测的结果会对近两年的节点比较敏感
-  如果寻则较大的k值，“学习”的近似误差会增大，与输入实例较远的训练实例也会对预测起作用，使预测发生错误。
- 实际中，k一般取一个比较小的数值，并采用交叉验证法来选取最优的k值
- k近邻法最简单的办法就是线性扫描$(linear scan)$，这时要计算输入实例和每一个训练实例的距离，当训练集很大时，非常耗时，这种方法不可行。

